{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_answer_dataset(questions,ground_truths,rag_chain,retriever):\n",
    "    \n",
    "    answers = []\n",
    "    contexts=[]\n",
    "    for query in questions:\n",
    "        answers.append(rag_chain.invoke(query))\n",
    "        contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "    data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def evaluate_rag(dataset)\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df = result.to_pandas()\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
